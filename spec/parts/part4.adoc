
== Part 4: pixel data

In this final part of the FLIF bitstream, the actual pixel values are encoded. There are two methods (or modes) of pixel encoding: non-interlaced and interlaced. The main header indicates which method is used (see Part 1 above).

=== Non-Interlaced method

If this encode method is used, then we start immediately with the encoding of the MANIAC trees (see below), followed by the encoding of the pixels. The order in which the pixels are encoded is described by the following nested loops:

* For all channels _c_ : (in the order 4,3,0,1,2, skipping those that don't exist or have a singleton range)
** For all rows _y_, i.e. for _y_ from 0 to **height**-1 :
*** For all frames _f_ , i.e. for _f_ from 0 to **nb_frames**-1 :
**** For all columns _x_, i.e. for _x_ from 0 to **width**-1 :
***** Encode the pixel at location (_x_,_y_) in channel _c_ of frame _f_

How the pixel is actually encoded is described below.

=== Interlaced method

For interlacing, we define the notion of _zoomlevels_. Zoomlevel 0 is the full image. Zoomlevel 1 are all the even-numbered rows of the image (counting from 0). Zoomlevel 2 are all the even-numbered columns of zoomlevel 1. In general: zoomlevel _2k+1_ are all the even-numbered rows of zoomlevel _2k_, and zoomlevel _2k+2_ are all the even-numbered columns of zoomlevel _2k+1_.

In other words, every even-numbered zoomlevel _2k_ is a downsampled version of the image, at scale 1:__2^k__.

We define the 'maximum zoomlevel' **max_zl** of an image as the zoomlevel with the lowest number that consists of a single pixel. This is always the pixel in the top-left corner of the image (row 0, column 0). This pixel is always encoded first, as a special case.

The zoomlevels are encoded from highest (most zoomed out) to lowest; in each zoomlevel, obviously only those pixels are encoded that haven't been encoded previously. So in an even-numbered zoomlevel, the odd-numbered rows are encoded, while in an odd-numbered zoomlevel, the odd-numbered columns are encoded.

If the interlaced encode method is used, we do not encode the MANIAC trees right away. Instead, we initialize the trees to a single root node per channel, and start encoding a 'rough preview' of the image (a few of the highest zoomlevels).
This allows a rough thumbnail extraction without needing to decode the MANIAC tree.
Then the MANIAC tree is encoded, and then the rest of the zoomlevels are encoded.


|===
| Type                                           | Description       | Condition

| uni_int(0..**max_zl**)                         | Number of the first MANIAC-encoded zoomlevel: **first_zl** |
| uni_int(range(_c_).min,range(_c_).max)         | pixel value of the top-left (0,0) pixel of channel _c_  | repeat(_c_ in 0..**nb_channels**-1)
| Encode_zoomlevels(**max_zl**-1,**first_zl**+1) | encoding of zoomlevels **max_zl**-1 until **first_zl**+1  |
| encoding of MANIAC trees                       | see further below    |
| Encode_zoomlevels(**first_zl**,0)              | encoding of zoomlevels **first_zl** until 0  |
|===

The encoding of a series of zoomlevels happens by interleaving the channels in some way. This interleaving is either in the 'default order', or in a custom order. In any case, the following invariants must hold:

* Zoomlevel _k_ of a channel can only be encoded after zoomlevel _k+1_ of that channel has already been encoded;
* If channel 3 exists and **alpha_zero** is true, then zoomlevel _k_ of channel 0 (usually Luma) can only be encoded after zoomlevel _k_ of channel 3 (Alpha) has already been encoded;
* Zoomlevel _k_ of channel 1 (usually Co) can only be encoded after zoomlevel _k_ of channel 0 (usually Luma) has been encoded;
* Zoomlevel _k_ of channel 2 (usually Cg) can only be encoded after zoomlevel _k_ of channels 0 and 1 (Luma and Co) have been encoded;
* If channel 4 (FrameLookback) exists: zoomlevel _k_ of any other channel (0,1,2, or 3) can only be encoded after zoomlevel _k_ of channel 4 has already been encoded.

TODO: describe default order

==== Encode_zoomlevels(h,l)

There are three different pixel predictors in interlaced mode (numbered 0, 1, 2). For each channel, either the same predictor is used in all zoomlevels h to l (in that case the predictor number is encoded only once), or a different predictor can be picked for each zoomlevel (in that case the number -1 is encoded, and the actual predictor number is encoded at the beginning of each zoomlevel).

|===
| Type                       | Description                        | Condition

| uni_int(0,1)               | Boolean: **default_order**         |
| uni_int(-1,2)              | Pixel predictors **pred[channel]** | repeat(**nb_channels**)
|===


Repeat ** nb_channels * (h-l+1) ** times: (once for every channel/zoomlevel in the range from **h** to **l**)

|===
| Type | Description | Condition | Default value

| uni_int(0,**nb_channels**-1)
| Channel **c** to be encoded
| not **default_order**
| given by default order

|
| Zoomlevel **z** is implicit
|
|

| uni_int(0,2)
| Pixel predictor **p** to use
| **pred[c]** == -1
| **pred[c]**

| Encode_zl(**c**,**z**,**p**)
| Encoding of the next zoomlevel of channel **c**
| **range(c).min < range(c).max** 
|
|===

The encoding of a single zoomlevel/channel combination is defined as follows:

Encode_zl(**c**,**z**,**p**) :

* If **z** is even (a 'horizontal' zoomlevel)
** For all _odd_ rows _y_ of zoomlevel **z**, i.e. for _y_ from 1 to (**height**-1)/(2^(**z**/2)) in steps of 2 :
*** For all frames _f_, i.e. for _f_ from 0 to **nb_frames**-1 :
**** For all columns _x_ of zoomlevel **z**, i.e. for _x_ from 0 to (**width**-1)/(2^(**z**/2)) :
***** Encode the pixel at location (_x_,_y_) in zoomlevel **z** of channel **c** of frame _f_, using predictor **p**
* Else (if **z** is odd, that is, a 'vertical' zoomlevel)
** For all rows _y_ of zoomlevel **z**, i.e. for _y_ from 0 to (**height**-1)/(2^(**z+1**/2)) :
*** For all frames _f_, i.e. for _f_ from 0 to **nb_frames**-1 :
**** For all _odd_ columns _x_ of zoomlevel **z**, i.e. for _x_ from 1 to (**width**-1)/(2^(**z**/2)) with steps of 2 :
***** Encode the pixel at location (_x_,_y_) in zoomlevel **z** of channel **c** of frame _f_, using predictor **p**

.Design rationale: Animation loop nesting
****
Both in the interlaced and in the non-interlaced mode, the frames of an animation are
interleaved at a relatively deep level of the nested loops.
This means that progressive decoding of an animation does not result in the first few frames at full quality,
but instead, you get all frames at reduced quality. In non-interlaced mode, early stages of progressive decoding
will give you the top portion of all frames, only in grayscale (assuming the YCoCg color transformation).
In interlaced mode, progressive decoding will give you a blurry / low resolution preview of the full animation.
****

=== Pixel encoding

The overall encoding mechanism of a pixel value _v_ is as follows:

* The value is predicted, resulting in a predicted value _guess_
* The range of possible values is computed as accurately as possible using **crange(c,...)**, resulting in _min_ and _max_
** _guess_ is adjusted to be a valid value in this range using **snap(c,...)**
* The local context of the pixel is computed as a MANIAC property vector _pvec_
* The number that actually gets encoded is the difference between the actual and predicted value _v_ - _guess_
** If the prediction is any good, these numbers tend to be close to zero
** To decode the value: nz_int_MANIAC~**c**,_pvec_~(_min_-_guess_, _max_-_guess_) + _guess_

The above mechanism is the same in interlaced and non-interlaced mode and for all channels; however the predictor (_guess_) and the layout of property vector (_pvec_) depends on the mode and the channel.

==== Skipped pixels

Some pixels are not encoded at all. There are three such cases:

1. If the **alpha_zero** flag is true, then 'invisible pixels' (pixels with Alpha value zero) semantically have undefined RGB values. So if the pixel value in channel 3 (Alpha) is equal to 0, then the pixel values in channels 0, 1, 2 (RGB, or e.g. YCoCg after transformations) are not encoded. However, the decoder does need to set the values of these pixels to _something_, since those 'invisible' values might very well be used in predictors or properties of neighboring _visible_ pixels. It sets these values simply to the predicted value. In interlaced mode, the predictor to use for that is called the 'invisible pixel predictor' (see part 3 above).
2. In animations which use the FrameShape transformation, from the second frame onwards, all rows have a 'begin' and 'end' column. Pixels before the begin and after the end are not encoded; their value is equal to the corresponding pixel value from the previous frame.
3. In animations which use the FrameLookback transformation, channel 4 (Lookback) refers to past frames. If the value in channel 4 is not zero but some number _k_ > 0, then all other channels are not encoded for that pixel. The pixel value for channels 0,1,2,3 is equal to the corresponding pixel value from _k_ frames ago.

TIP: As an optimization, it is safe to simply skip pixels from a constant channel, that is, a channel which has a singleton range. Technically, one could argue that they are actually encoded, but for each pixel value _v_, we have _v_ = _min_ = _max_ = _guess_, which means the symbol encoding doesn't use any bits.

==== Pixel predictors

The different pixel predictors are described below. In each case, the actual predicted value is the value returned by the *snap* function. Usually the *snap* function will simply return its input value, but not always. Noteworthy exceptions are Predictor 1 of Interlaced mode, which as defined below could even be outside the channel range *range*, and any predictor in combination with the ColorBuckets transformation, which can have a nontrivial *snap* function.

===== Non-interlaced mode

Assume we have to predict pixel *X* in channel *c* given its previously decoded neighbors:

[%autowidth]
|===
| *TL* | *T* 
| *L*  | *X* 
|===

The predicted value for *X* is the median of the following three values:

* *L* + *T* - *TL*
* *L*
* *T*

Border conditions:

* If *X* is at column 0 (so it has no left neighbor *L*), then we set *L* = **range(c).min**, _except_ when *alpha_zero* is true, *c* < 3 and the alpha value at the position of pixel *X* is zero; in that case we set *L* = (**range(c).min**+**range(c).max**)/2.
* If *X* is at row 0 (so it has no top neighbor *T*), then we set *T* = *TL* = *L*.
* If *X* is at row > 0 and column 0, then *TL* = *T*.



===== Interlaced mode

In interlaced mode, there are three different predictors. Moreover, the predictors are slightly different in even-numbered zoomlevels compared to odd-numbered zoomlevels, since slightly different neighboring pixels are available (already decoded) : in horizontal zoomlevels the pixel _below_ the current pixel is already known (since it is part of the previously decoded zoomlevel), while in vertical zoomlevels the pixel _to the right_ of the current pixel is already known.

====== Horizontal zoomlevel (even-numbered)

Assume we have to predict pixel *X* in channel *c* given its previously decoded neighbors.

WARNING: These neighbors are defined with respect to the current zoomlevel. In the full image, these 'neighbors' are not directly adjacent (except in zoomlevel 0).

[%autowidth]
|===
|      |      | *TT* |
|      | *TL* | *T*  | *TR*
| *LL* | *L*  | *X*  |
|      | *BL* | *B*  | *BR*
|===

The following predictors are defined:

Predictor 0::
(*T* + *B*)>>1

Predictor 1:: 
The median of the following three values:

* (*T* + *B*)>>1
* *L* + *T* - *TL*
* *L* + *B* - *BL*

Predictor 2::
The median of *T*, *B* and *L*

Border conditions:

* If *X* is at column 0 (so it has no left neighbor *L*), then we set *L* = *TL* = *BL* = *T*
* If *X* is at the rightmost column, then we set *TR* = *T*.
* If *X* is at the last row, then *BL* = *B* = *L*.
* If *X* is at the rightmost column or last row, then *BR* = *B*.

====== Vertical zoomlevel (odd-numbered)

Assume we have to predict pixel *X* in channel *c* given its previously decoded neighbors.

[%autowidth]
|===
|      |      | *TT* |
|      | *TL* | *T*  | *TR*
| *LL* | *L*  | *X*  | *R*
|      | *BL* |      | *BR*
|===

The following predictors are defined:

Predictor 0 (**P~0~**)::
(*L* + *R*)>>1

Predictor 1 (**P~1~**):: 
The median of the following three values:

* (*L* + *R*)>>1
* *L* + *T* - *TL*
* *R* + *T* - *TR*

Predictor 2 (**P~2~**)::
The median of *T*, *L* and *R*

Border conditions:

* If *X* is at row 0 (so it has no top neighbor *T*), then we set *T* = *TL* = *TR* = *L*
* If *X* is at the rightmost column, then we set *TR* = *R* = *T*.
* If *X* is at the last row, then *BL* = *L*.
* If *X* is at the rightmost column or last row, then *BR* = *R*.


==== Properties

The MANIAC property vector which is used to determine the context (i.e. the MANIAC tree leaf node) is constructed
in the following way. Assume the pixel currently being decoded is pixel *X* in channel number **c** and the pixel values of previously decoded neighboring pixels (of the same channel) are named as in the diagram below -- it depends on the interlacing mode and zoomlevel which of these are known at decode time.

[%autowidth]
|===
|      |      | *TT* |
|      | *TL* | *T*  | *TR*
| *LL* | *L*  | *X*  | *R*
|      | *BL* | *B*  |
|===

Moreover, we use **X~pc~** to denote the pixel value at the position of *X* in a previously decoded channel *pc*,
and **P** to denote the predicted value for *X* (as described above).

Finally, we define the range **maxdiff(c)** as the range of a difference between two pixel values in channel *c*, that is: **maxdiff(c).min** = **range(c).min** - **range(c).max** and **maxdiff(c).max** = **range(0).max** - **range(c).min**.

===== Non-interlaced mode

In non-interlaced mode, the following MANIAC properties are defined for a pixel in channel *c*:

|===
| Property    | Description            | Condition     | Property range (*prange*)

| **X~0~**    | Luma / channel 0 value | 0 < **c** < 3 | **range(0)**
| **X~1~**    | Co / channel 1 value   | 1 < **c** < 3 | **range(1)**
| **X~3~**    | Alpha value            | **c** < 3 < **nb_channels** | **range(3)**
| **P**       | Predicted value **snap(c,...,**median(*L*+*T*-*TL*,*L*,*T*)**)**      |               | **range(c)**
| **M~ni~**   | Median-index (see below) |               | 0..2
| *L* - *TL*  | Left - Topleft         |               | **maxdiff(c)**
| *TL* - *T*  | Topleft - Top          |               | **maxdiff(c)**
| *T* - *TR*  | Top - Topright         |               | **maxdiff(c)**
| *TT* - *T*  | Toptop - Top           |               | **maxdiff(c)**
| *LL* - *L*  | Leftleft - Left        |               | **maxdiff(c)**
|===

NOTE: For images _without_ an alpha channel, channel 0 has 7 properties (numbered 0 to 6), channel 1 has 8 properties, and channel 2 has 9 properties. For images with an alpha channel, channel 0 has 8 properties, channel 1 has 9 properties, channel 2 has 10 properties, and channel 3 has 7 properties. In animations with FrameLookback, channel 4 has 7 properties.

Border conditions: if in the last five properties, which are differences between two neighboring pixels, one of those pixels does not exist, the property is defined to be zero.

The median-index **M~ni~** indicates which of the three input values for the median predictor became the predictor; more precisely, it is defined as follows:

**M~ni~** =

[horizontal]
0 :: if *P* = *L*+*T*-*TL*
1 :: if *P* = *L*  (and the above condition does not hold)
2 :: if *P* = *T*  (and neither of the above conditions hold)
0 :: otherwise

===== Interlaced mode

In interlaced mode, the following MANIAC properties are defined for a pixel in zoomlevel *z* of channel *c* using predictor *p*:

|===
| Property    | Description            | Condition     | Property range (*prange*)

| **X~0~**    | Luma / channel 0 value | 0 < **c** < 3 | **range(0)**
| **X~1~**    | Co / channel 1 value   | 1 < **c** < 3 | **range(1)**
| **X~3~**    | Alpha value            | **c** < 3 < **nb_channels** | **range(3)**
| **M~i~**    | Median-index (see below) |             | 0..2
| *X~0~* - (*T~0~*+*B~0~*)>>1  | Luma 'prediction miss' | 0 < *c* < 3 and *z* is even | **maxdiff(0)**
| *X~0~* - (*L~0~*+*R~0~*)>>1  | Luma 'prediction miss' | 0 < *c* < 3 and *z* is odd | **maxdiff(0)**
| *T* - *B*   | Top - Bottom           | *z* is even   | **maxdiff(c)**
| *T* - (*TL* + *TR*)>>1 | Top 'prediction miss'  | *z* is even | **maxdiff(c)**
| *L* - (*BL* + *TL*)>>1 | Left 'prediction miss' | *z* is even | **maxdiff(c)**
| *B* - (*BL* + *BR*)>>1 | Bottom 'prediction miss' | *z* is even | **maxdiff(c)**
| *L* - *R*   | Left - Right           | *z* is odd    | **maxdiff(c)**
| *L* - (*BL* + *TL*)>>1 | Left 'prediction miss' | *z* is odd | **maxdiff(c)**
| *T* - (*TL* + *TR*)>>1 | Top 'prediction miss'  | *z* is odd | **maxdiff(c)**
| *R* - (*BR* + *TR*)>>1 | Right 'prediction miss' | *z* is odd | **maxdiff(c)**
| **P~p~**    | Predicted value (see above) |          | **range(c)**
| *TT* - *T*  | Toptop - Top           | *c* is not 2  | **maxdiff(c)**
| *LL* - *L*  | Leftleft - Left        | *c* is not 2  | **maxdiff(c)**
|===

NOTE: For images _without_ an alpha channel, channel 0 has 8 properties (numbered 0 to 7), channel 1 has 10 properties, and channel 2 has 9 properties. For images with an alpha channel, channel 0 has 9 properties, channel 1 has 11 properties, channel 2 has 10 properties, and channel 3 has 8 properties. In animations with FrameLookback, channel 4 has 8 properties.

Border conditions:
If *TL*, *T*, *TR*, *L*, *BL*, *B*, *BR* or *R* are missing, they are defined as described in the interlaced pixel predictor definition above. If *TT* or *LL* are missing, the property they occur in gets value zero.
If *B~0~* is missing, *B~0~* = *T~0~*; if *R~0~* is missing, *R~0~* = *L~0~*.

The median-index **M~i~** indicates which of the three input values for the median in Predictor 1 is the median; more precisely, it is defined as follows:

**M~i~** =

[horizontal]
0 :: if *z* is even and **P~1~** = (*T* + *B*)>>1
0 :: if *z* is odd and **P~1~** = (*L* + *R*)>>1
1 :: if **P~1~** = *L* + *T* - *TR*  (and the above conditions do not hold)
2 :: otherwise


=== MANIAC tree encoding
There is one tree per non-trivial channel (a channel is trivial if its range is a singleton or if it doesn't exist).
The trees are encoded one after another and in a recursive (depth-first) way, as follows:

**nb_properties** depends on the channel, the number of channels, and the encoding method (interlaced or non-interlaced),
as specified above. The range of each property is maintained during tree traversal. The initial property ranges
**prange[_property_]** are defined above; these are narrowed down when going to a deeper node in the tree.

For each channel _c_, three different contexts are used: we'll just call them A~_c_~, B~_c_~ and C~_c_~.

|===
| Type | Description | Condition

| nz_int_A~_c_~(0,**nb_properties**)
| 0=leaf node, > 0: _property_+1
|

| nz_int_B~_c_~(1,512)
| node counter
| not a leaf node

| nz_int_C~_c_~(**prange[_property_].min**,**prange[_property_].max**-1)
| _test_value_
| not a leaf node

| recursive encoding of left branch
| where **prange[_property_].min** = _test_value_+1
| not a leaf node

| recursive encoding of right branch
| where **prange[_property_].max** = _test_value_
| not a leaf node
|===


From this description, the MANIAC trees can be reconstructed. Leaf nodes have a counter value that is effectively infinity
(they can never be turned into a decision node).


=== Checksum

At the very end of the bitstream, there is an optional checksum to verify the integrity of the file.

TODO: describe how the checksum is computed

|===
| Type             | Description                       | Condition

| uni_int(1)       | Boolean: **have_checksum**            |
| uni_int(16)      | Most significant 16 bits of checksum  | **have_checksum**
| uni_int(16)      | Least significant 16 bits of checksum | **have_checksum**
|===
